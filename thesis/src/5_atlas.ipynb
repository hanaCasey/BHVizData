{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdfkit\n",
    "import glob\n",
    "import os\n",
    "import base64\n",
    "import pdfkit\n",
    "import module\n",
    "import ast\n",
    "\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfMerger\n",
    "import multidict as multidict\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.interpolate import make_interp_spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query OpenAI for title and description of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY= ''\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Atlas from titles and description\n",
    "\n",
    "Making one page per cluster with wordclouds and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = 'utf-8' \n",
    "\n",
    "# embedding of books\n",
    "embedding_i = pd.read_csv('data/internal_embedding.csv', encoding=encoding, encoding_errors='replace')\n",
    "embedding_i  = module.eval_as(embedding_i)\n",
    "\n",
    "# clusters\n",
    "clusters_i = pd.read_csv('data/clusters_i.csv', encoding=encoding, encoding_errors='replace')\n",
    "# top 100 loaned books in cluster\n",
    "clusters_i['top_titles'] = embedding_i.sort_values(['frequency_norm']).groupby('cluster').head(100).groupby('cluster').title.apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan frequency of top titles per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loans of top titles in each cluster\n",
    "frequencies = []\n",
    "\n",
    "# Get only top titles' loan frequency\n",
    "for cluster_id, row in clusters_i.iterrows():\n",
    "\n",
    "    top_titles = row.top_titles\n",
    "    books = embedding_i[(embedding_i.cluster == cluster_id) & (embedding_i.title.isin(top_titles))]\n",
    "    cluster_frequencies = books['yearly_frequency'].aggregate(module.sum_loan_times)\n",
    "    frequencies.append(cluster_frequencies)\n",
    "\n",
    "cluster_yearly_time_df = pd.DataFrame(frequencies).fillna(0)\n",
    "sorted_columns = sorted(cluster_yearly_time_df.columns)\n",
    "x = np.arange(len(sorted_columns))\n",
    "\n",
    "for cluster_id, row in cluster_yearly_time_df.iterrows():\n",
    "\n",
    "    plt.figure(figsize=(5, 3), dpi=300)\n",
    "\n",
    "    spl = make_interp_spline(x, row, k=3)\n",
    "    x_new = np.linspace(0, len(sorted_columns) - 1, 300)\n",
    "    y_new = spl(x_new)\n",
    "\n",
    "    plt.fill_between(x_new, y_new, color=module.CUSTOM_COLOURS[10])\n",
    "    plt.title(f'Cluster {cluster_id}: Loans per year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Loans')\n",
    "    plt.ylim(0, max(row) + (max(row)*0.3))\n",
    "\n",
    "    plt.xticks(x, sorted_columns, rotation=45)  # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(f'exp/loans/cluster_{cluster_id}_loans.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordclouds for each cluster in top four languages\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "feature_frequencies = []\n",
    "\n",
    "# List of available stopwords \n",
    "english_stop_words = list(stopwords.words('english'))\n",
    "german_stop_words = list(stopwords.words('german'))\n",
    "italian_stop_words = list(stopwords.words('italian'))\n",
    "french_stop_words = list(stopwords.words('french'))\n",
    "\n",
    "wc = WordCloud( \n",
    "        mode = \"RGBA\",\n",
    "        color_func=lambda *args, **kwargs: (0, 0, 0),\n",
    "        font_path = path.join('Lato-Regular.ttf'),\n",
    "        # mask=mask,\n",
    "        normalize_plurals=False,\n",
    "        prefer_horizontal= 1,\n",
    "        margin=10,\n",
    "        background_color=None,\n",
    "        # background_color='black',\n",
    "        # max_words=max_words,\n",
    "        min_font_size= 5,\n",
    "        max_font_size= 100,\n",
    "        # collocation_threshold = 20,\n",
    "        relative_scaling = 1,\n",
    "    )\n",
    "\n",
    "clusters = embedding_i[embedding_i.cluster != -1].groupby('cluster')\n",
    "\n",
    "for cluster_id, cluster in clusters:\n",
    "    nr_langs = 4\n",
    "    langs = cluster.groupby('lang').size().head(nr_langs).reset_index()['lang'].tolist()\n",
    "    lang_titles = cluster[cluster['lang'].isin(langs)].groupby('lang')\n",
    "\n",
    "    cluster_frequencies = []\n",
    "    \n",
    "    for lang, lang_group in lang_titles:\n",
    "        # Remove stop words for main languages\n",
    "        if lang == 'eng':\n",
    "            stop_words = english_stop_words\n",
    "        elif lang == 'ger':\n",
    "            stop_words = german_stop_words\n",
    "        elif lang == 'ita':\n",
    "            stop_words = italian_stop_words\n",
    "        elif lang == 'fre':\n",
    "            stop_words = french_stop_words\n",
    "        else:\n",
    "            stop_words = None\n",
    "\n",
    "        # Apply stop words removal and tfidf vectorizer\n",
    "        titles = ' '.join(lang_group['title'].tolist())\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "        tfidf_matrix = vectorizer.fit_transform([titles])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Generate word cloud\n",
    "        wc.generate_from_frequencies(dict(zip(feature_names, tfidf_matrix.toarray()[0])))\n",
    "        wc.to_file(path.join(\"exp/wc/\", f\"{cluster_id}_{lang}.png\"))\n",
    "        \n",
    "        cluster_frequencies.append((dict(zip(feature_names, tfidf_matrix.toarray()[0]))))\n",
    "    \n",
    "    feature_frequencies.append(cluster_frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the top 100 words in the dictionaries per language\n",
    "top_n = 100\n",
    "for cluster_idx, cluster_freq in enumerate(feature_frequencies):\n",
    "    for idx, freq_dict in enumerate(cluster_freq):\n",
    "        sorted_words = dict(sorted(freq_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Keep only the top N most frequent words\n",
    "        sorted_words = {k: v for k, v in sorted_words.items() if v > 0}  # Remove words with frequency 0\n",
    "        sorted_words = dict(list(sorted_words.items())[:top_n])  # Keep only the top N\n",
    "        cluster_freq[idx] = sorted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_i['wc_frequencies'] = feature_frequencies\n",
    "# Rounding the decimal places\n",
    "decimal_places = 4\n",
    "clusters_i['wc_frequencies'] = clusters_i['wc_frequencies'].apply(lambda cluster_frequencies: [{key: round(value, decimal_places) for key, value in lang_frequencies.items()} for lang_frequencies in cluster_frequencies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cluster title from word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_from_frequencies(frequencies):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Based on the following terms and tf-idf scores, suggest a subject classification (up to 10 words) for the associated cluster of books. Give me the response in format: Subject: subject\"}, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"These are the terms and frequencies: {frequencies}\"\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def extract_titles (text): \n",
    "\n",
    "    split_text = text.split('\\n\\n')\n",
    "    title = split_text[0]\n",
    "    if(title.startswith('Subject:')):\n",
    "        return(title[8:])\n",
    "    else: \n",
    "        return title\n",
    "\n",
    "\n",
    "# Getting a description like this absolutely doesn't work. \n",
    "# The number of input clusters doens't correspond to the output. \n",
    "# And then it gives random subject like \n",
    "#  Cluster 1: History and Geography)\n",
    "#  Cluster 2: Advanced Mathematics and Algorithms)\n",
    "#  Cluster 3: Food Science and Culinary Concepts)\n",
    "#  Cluster 4: Psychological Theory and Therapy)\n",
    "    \n",
    "def get_subject_vocabulary(df):\n",
    "    frequencies = df.wc_frequencies\n",
    "\n",
    "    try:\n",
    "\n",
    "        initial_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Based on the following terms and frequencies, suggest a meaningful subject classification (up to 10 words) for the associated cluster of books. Give me the response in format cluster output\"\n",
    "        }\n",
    "        messages = [initial_message]\n",
    "        for _, row in df.iterrows():\n",
    "            cluster_id = row['cluster']\n",
    "            frequencies = row['wc_frequencies']\n",
    "\n",
    "            cluster_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"START Cluster {cluster_id}:{frequencies}. END Cluster {cluster_id}\"\n",
    "            }\n",
    "\n",
    "            completion = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = clusters_i.wc_frequencies.apply(get_title_from_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_i['subject_output'] = response\n",
    "clusters_i['cluster_subject'] = clusters_i['subject_output'].apply(extract_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_from_titles(cluster_title, titles):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Based on the following cluster subject and book titles, suggest a description (up to 100 words) for the associated cluster of books. Give me the response in format: Description: description\"}, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"This is the cluster subject : {cluster_title}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"These are the book titles : {titles}\"\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def extract_descriptions(text): \n",
    "\n",
    "    split_text = text.split('\\n\\n')\n",
    "    description = split_text[0]\n",
    "    start = 'Description:'\n",
    "    if(description.startswith(start)):\n",
    "        return(description[len(start):])\n",
    "    else: \n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_i['description_output'] = clusters_i.apply(lambda row: get_description_from_titles(row['cluster_subject'], row['top_titles']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_i['cluster_description'] = clusters_i.description_output.apply(extract_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the clusters on the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map for each cluster with cluster in red and other clusters in blue\n",
    "\n",
    "df = embedding_i\n",
    "\n",
    "unique_clusters = np.unique(df['cluster'])\n",
    "unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "for cluster_number in unique_clusters:\n",
    "    # print(cluster_number)\n",
    "    plt.figure(figsize=(10, 5), dpi=300)\n",
    "    cluster_indices = np.where(df['cluster'] == cluster_number)[0]\n",
    "    other_indices = np.where(df['cluster'] != cluster_number)[0]\n",
    "    cluster_x = df.iloc[cluster_indices]['x'].mean()\n",
    "    cluster_y = df.iloc[cluster_indices]['y'].mean()\n",
    "    \n",
    "    # Scatter plot for the current cluster with a specified color\n",
    "    plt.scatter(df.iloc[cluster_indices]['x'], df.iloc[cluster_indices]['y'], s=5, c=module.CUSTOM_COLOURS[0])\n",
    "    plt.scatter(df.iloc[other_indices]['x'], df.iloc[other_indices]['y'], s=2, c=module.CUSTOM_COLOURS[10])\n",
    "    \n",
    "    # Text label for the cluster number\n",
    "    plt.text(cluster_x , cluster_y + 20, str(cluster_number), fontsize=16, ha='center')\n",
    "    # plt.title(f'Cluster {cluster_number}')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'exp/maps/cluster_{cluster_number}_map.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate html and pdf for Cluster Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_directory = os.path.abspath('export.css')\n",
    "\n",
    "def generate_pdf_for_clusters(clusters):\n",
    "    for _, cluster in clusters.iterrows():\n",
    "        cluster_id = cluster['cluster']\n",
    "        title = cluster['cluster_subject']\n",
    "        description = cluster['cluster_description']\n",
    "        top_titles = cluster['top_titles']\n",
    "\n",
    "        image_pattern = f\"exp/wc/{cluster_id}_*.png\"\n",
    "        image_paths = [os.path.abspath(image) for image in glob.glob(image_pattern)]\n",
    "\n",
    "        pdf_directory = \"exp/pdfs\"\n",
    "        pdf_filename = f\"{pdf_directory}/cluster_{cluster_id}_report.pdf\"\n",
    "        \n",
    "        loans_directory = \"exp/loans\"\n",
    "        loans_path = f\"{loans_directory}/cluster_{cluster_id}_loans.png\"\n",
    "\n",
    "        maps_directory = \"exp/maps\"\n",
    "        maps_path = f\"{maps_directory}/cluster_{cluster_id}_map.png\"\n",
    "\n",
    "        # Create an HTML template for each cluster\n",
    "        html_template = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <title>{cluster_id} - {title}</title>\n",
    "            <link rel=\"stylesheet\" type=\"text/css\" href=\"{css_directory}\">\n",
    "        </head>\n",
    "        <body>\n",
    "        <div class='page-wrapper'>\n",
    "                \n",
    "        <h1>{cluster_id} - {title}</h1>\n",
    "        <p>{description}</p>\n",
    "\n",
    "        <div class=\"row\">\n",
    "        \"\"\"\n",
    "        if os.path.exists(loans_path):\n",
    "            data_uri = base64.b64encode(open(loans_path, 'rb').read()).decode('utf-8')\n",
    "            img_tag = '<div class=\"loan-figure\"><img src=\"data:image/png;base64,{0}\"></div>'.format(data_uri)\n",
    "            html_template += img_tag\n",
    "        else:\n",
    "            print(f\"Image not found: {loans_path}\")\n",
    "\n",
    "        if os.path.exists(maps_path):\n",
    "            data_uri = base64.b64encode(open(maps_path, 'rb').read()).decode('utf-8')\n",
    "            img_tag = '<div class=\"loan-figure\"><img src=\"data:image/png;base64,{0}\"></div>'.format(data_uri)\n",
    "            html_template += img_tag\n",
    "\n",
    "        else:\n",
    "            print(f\"Image not found: {maps_path}\")\n",
    "\n",
    "        html_template += \"\"\"\n",
    "        </div>\n",
    "        \n",
    "        <div class=wordclouds>\n",
    "        \"\"\"\n",
    "\n",
    "        # Add images in a row\n",
    "        for image_path in image_paths[:4]:\n",
    "            if os.path.exists(image_path):\n",
    "                data_uri = base64.b64encode(open(image_path, 'rb').read()).decode('utf-8')\n",
    "                img_tag = '<img class=\"wordcloud-image\" src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                html_template += img_tag\n",
    "            else:\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "\n",
    "\n",
    "        html_template += \"\"\"\n",
    "        </div>\n",
    "\n",
    "        <div class=\"page-break\"></div> <!-- Page break to ensure titles start on a new page -->\n",
    "        <div class=row>\n",
    "        \"\"\"\n",
    "\n",
    "         # Add the top titles in two columns\n",
    "        half_point = len(top_titles) // 2\n",
    "        for i, top_title in enumerate(top_titles):\n",
    "            if i == 0 or i == half_point:\n",
    "                html_template += '<div class=\"column\">'\n",
    "            html_template += f\"<p>{top_title}</p>\"\n",
    "            if i == half_point - 1 or i == len(top_titles) - 1:\n",
    "                html_template += '</div>'\n",
    "\n",
    "        html_template += \"\"\"\n",
    "        </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        options = {\n",
    "            'enable-local-file-access': True,\n",
    "            \"page-size\": \"A4\",\n",
    "            \"user-style-sheet\": css_directory,\n",
    "            'encoding': \"UTF-8\"\n",
    "        }\n",
    "        # Generate PDF for the cluster\n",
    "        pdfkit.from_string(html_template, pdf_filename, options=options)\n",
    "\n",
    "clusters_i = pd.read_csv('exp/cluster_atlas_full.csv', encoding='utf-8', encoding_errors='replace')\n",
    "clusters_i.top_titles = clusters_i.top_titles.apply(ast.literal_eval)\n",
    "generate_pdf_for_clusters(clusters_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing HTML files\n",
    "html_directory = \"exp/html\"\n",
    "\n",
    "# Directory to save the final PDF\n",
    "output_pdf = \"exp/cluster_atlas_full.pdf\"\n",
    "\n",
    "pdf_directory = \"exp/pdfs\"\n",
    "pdf_files = [os.path.join(pdf_directory, filename) for filename in os.listdir(pdf_directory) if filename.endswith(\".pdf\")]\n",
    "\n",
    "# Sort the PDF files by their cluster number\n",
    "pdf_files = sorted(pdf_files, key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[1]))\n",
    "\n",
    "# Merge the individual PDFs into one PDF file\n",
    "merger = PdfMerger()\n",
    "for pdf_file in pdf_files:\n",
    "    merger.append(pdf_file)\n",
    "    \n",
    "# Save the merged PDF\n",
    "merger.write(output_pdf)\n",
    "merger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_i.to_csv('exp/cluster_atlas_full.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhviz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
